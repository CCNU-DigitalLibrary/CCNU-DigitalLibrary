# encoding: utf-8
import copy
import itertools
from collections import defaultdict
from typing import Optional, List

import numpy as np
from torch.utils.data.sampler import Sampler

from lib.utils import comm


def no_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j != b]


def reorder_index(batch_indices, world_size):
    r"""Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:

    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


class BalancedIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of dict('image_id', 'id', 'file_path', 'sentence', ...).
    - images_per_pid (int): number of instances per identity in a batch.
    - mini_batch_size (int): number of examples in a gpu.
    """
    def __init__(self, data_source: List, mini_batch_size: int, images_per_pid: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.images_per_pid = images_per_pid
        self.num_pids_per_gpu = mini_batch_size // self.images_per_pid

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.index_pid = dict()
        self.pid_image = defaultdict(list)
        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info['id']
            image_id = info['image_id']
            self.index_pid[index] = pid
            self.pid_image[pid].append(image_id)
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)
        self._can_reuse_indices = False

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self.epoch = 0

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        if self._can_reuse_indices:
            _indices = self._indices
            self._can_reuse_indices = False
        else:
            _indices = self._prepare_indices()
        self._indices = _indices

        yield from itertools.islice(self._indices, start, None, self._world_size)

    def _prepare_indices(self):
        np.random.seed(self._seed + self.epoch)
        identities = np.random.permutation(self.num_identities)

        all_indices = []
        _indices = []
        for kid in identities:
            i = np.random.choice(self.pid_index[self.pids[kid]])
            i_image = self.data_source[i]['image_id']

            pid_i = self.index_pid[i]
            images = self.pid_image[pid_i]
            index = self.pid_index[pid_i]
            select_images = no_index(images, i_image)

            if select_images:
                _indices.append(i)
                if len(select_images) >= self.images_per_pid:
                    image_indexes = np.random.choice(select_images, size=self.images_per_pid - 1, replace=False)
                else:
                    image_indexes = np.random.choice(select_images, size=self.images_per_pid - 1, replace=True)
                for kk in image_indexes:
                    _indices.append(index[kk])
            else:
                select_indexes = no_index(index, i)
                if not select_indexes:                        
                    # Only one image for this identity
                    continue
                elif len(select_indexes) >= self.images_per_pid:
                    _indices.append(i)
                    ind_indexes = np.random.choice(select_indexes, size=self.images_per_pid - 1, replace=False)
                else:
                    _indices.append(i)
                    ind_indexes = np.random.choice(select_indexes, size=self.images_per_pid - 1, replace=True)

                for kk in ind_indexes:
                    _indices.append(index[kk])

            if len(_indices) == self.batch_size:
                all_indices.extend(reorder_index(_indices, self._world_size))
                _indices = []
        
        return all_indices

    def __len__(self):
        if not hasattr(self, '_indices'):
            self._indices = self._prepare_indices()
            self._can_reuse_indices = True
        return len(self._indices) // self._world_size

    def set_epoch(self, epoch: int) -> None:
        r"""
        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
        use a different random ordering for each epoch. Otherwise, the next iteration of this
        sampler will yield the same ordering.

        Args:
            epoch (int): Epoch number.
        """
        self.epoch = epoch


class NaiveIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - images_per_pid (int): number of instances per identity in a batch.
    - mini_batch_size (int): number of examples in a gpu.
    """

    def __init__(self, data_source: List, mini_batch_size: int, images_per_pid: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.images_per_pid = images_per_pid
        self.num_pids_per_gpu = mini_batch_size // self.images_per_pid

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info['id']
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)
        self._can_reuse_indices = False

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self.epoch = 0

    def __iter__(self):
        start = self._rank
        if self._can_reuse_indices:
            _indices = self._indices
            self._can_reuse_indices = False
        else:
            _indices = self._prepare_indices()
        self._indices = _indices

        yield from itertools.islice(self._indices, start, None, self._world_size)

    def _prepare_indices(self):
        np.random.seed(self._seed + self.epoch)
        avl_pids = copy.deepcopy(self.pids)
        batch_idxs_dict = {}

        all_indices = []
        while len(avl_pids) >= self.num_pids_per_gpu:
            _indices = []
            selected_pids = np.random.choice(avl_pids, self.num_pids_per_gpu, replace=False).tolist()
            for pid in selected_pids:
                # Register pid in batch_idxs_dict if not
                if pid not in batch_idxs_dict:
                    idxs = copy.deepcopy(self.pid_index[pid])
                    if len(idxs) < self.images_per_pid:
                        idxs = np.random.choice(idxs, size=self.images_per_pid, replace=True).tolist()
                    np.random.shuffle(idxs)
                    batch_idxs_dict[pid] = idxs

                avl_idxs = batch_idxs_dict[pid]
                for _ in range(self.images_per_pid):
                    _indices.append(avl_idxs.pop(0))

                if len(avl_idxs) < self.images_per_pid: 
                    avl_pids.remove(pid)

            all_indices.extend(reorder_index(_indices, self._world_size))
        return all_indices

    def __len__(self):
        if not hasattr(self, '_indices'):
            self._indices = self._prepare_indices()
            self._can_reuse_indices = True
        return len(self._indices) // self._world_size

    def set_epoch(self, epoch: int) -> None:
        r"""
        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
        use a different random ordering for each epoch. Otherwise, the next iteration of this
        sampler will yield the same ordering.

        Args:
            epoch (int): Epoch number.
        """
        self.epoch = epoch